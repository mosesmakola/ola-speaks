{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mosesmakola/ola-speaks/blob/emmanuel-dev/TrainandTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers datasets huggingface_hub snowflake-connector-python pandas numpy scikit-learn tqdm tensorboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSGc9TGwG_bL",
        "outputId": "5ad0ec22-beba-4680-c77d-6a43c736ac15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.1)\n",
            "Collecting snowflake-connector-python\n",
            "  Downloading snowflake_connector_python-3.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n",
            "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (1.17.1)\n",
            "Requirement already satisfied: cryptography>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (43.0.3)\n",
            "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (24.2.1)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.10.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2025.1.31)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.4.0)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (4.3.7)\n",
            "Collecting tomlkit (from snowflake-connector-python)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading snowflake_connector_python-3.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: asn1crypto, xxhash, tomlkit, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, snowflake-connector-python, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asn1crypto-1.5.1 datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 snowflake-connector-python-3.14.0 tomlkit-0.13.2 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install bitsandbytes tokenizers snowflake-connector-python[pandas]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ruqZJrrHXis",
        "outputId": "e66848da-c9fd-41c0-f50f-2a55e92d8655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
            "Requirement already satisfied: snowflake-connector-python[pandas] in /usr/local/lib/python3.11/dist-packages (3.14.0)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.30.1)\n",
            "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (1.5.1)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (1.17.1)\n",
            "Requirement already satisfied: cryptography>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (43.0.3)\n",
            "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (24.2.1)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (2.10.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (2025.2)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (24.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions<5,>=4.3 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (4.13.1)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (3.18.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (2.4.0)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (4.3.7)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (0.13.2)\n",
            "Requirement already satisfied: pandas<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<19.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (18.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python[pandas]) (2.22)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=1.0.0->snowflake-connector-python[pandas]) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=1.0.0->snowflake-connector-python[pandas]) (2025.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0->snowflake-connector-python[pandas]) (2.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=1.0.0->snowflake-connector-python[pandas]) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install sacrebleu nltk"
      ],
      "metadata": {
        "id": "V45HWrLmNSE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All imports\n",
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from multiprocessing import Pool\n",
        "from tqdm.auto import tqdm\n",
        "from huggingface_hub import login\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainer\n",
        ")\n",
        "from datasets import Dataset, load_from_disk\n",
        "from sklearn.model_selection import train_test_split\n",
        "import snowflake.connector\n",
        "from snowflake.connector.pandas_tools import write_pandas\n",
        "import sacrebleu\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
      ],
      "metadata": {
        "id": "AdMELrFqHj-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"nllb_training.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "Yd7eppXgHkMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Login to Hugging Face\n",
        "logger.info(\"Logging into Hugging Face Hub\")\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "# Check for CUDA availability\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "logger.info(f\"Using device: {device}\")\n",
        "\n",
        "# Paths for cached datasets\n",
        "CACHE_DIR = \"./cached_datasets\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "TRAIN_CACHE = os.path.join(CACHE_DIR, \"train_dataset\")\n",
        "VAL_CACHE = os.path.join(CACHE_DIR, \"val_dataset\")\n",
        "TEST_CACHE = os.path.join(CACHE_DIR, \"test_dataset\")"
      ],
      "metadata": {
        "id": "sD8Dz7F4HkTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "logger.info(\"Loading model and tokenizer\")\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load model with optimizations for training\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None,\n",
        "    torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,)\n",
        "\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "Enn53vWNHkWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SF_USER = userdata.get(\"SF_USER\")\n",
        "SF_PASSWORD = userdata.get(\"SF_PASSWORD\")\n",
        "SF_ACCOUNT = userdata.get(\"SF_ACCOUNT\")"
      ],
      "metadata": {
        "id": "v0Yk31O3PSTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_data_in_batches(batch_size=10000, max_rows=None):\n",
        "    \"\"\"Fetch data from Snowflake in batches\"\"\"\n",
        "    logger.info(\"Connecting to Snowflake\")\n",
        "    conn = snowflake.connector.connect(\n",
        "        user=SF_USER,\n",
        "        password=SF_PASSWORD,\n",
        "        account=SF_ACCOUNT,\n",
        "        warehouse='OLASPEAKS',\n",
        "        database='TEXT_LANGUAGE_DATA',\n",
        "        schema='PUBLIC',\n",
        "    )\n",
        "    cur = conn.cursor()\n",
        "    offset = 0\n",
        "    all_rows = []\n",
        "\n",
        "    logger.info(\"Fetching data in batches\")\n",
        "    while True:\n",
        "        cur.execute(f\"SELECT ENG, YOR, LIN FROM TEXT_LANGUAGE_DATA.BIBLE.RAW_BIBLE LIMIT {batch_size} OFFSET {offset}\")\n",
        "        batch = cur.fetchall()\n",
        "        if not batch:\n",
        "            break\n",
        "        all_rows.extend(batch)\n",
        "        offset += batch_size\n",
        "        logger.info(f\"Fetched {len(all_rows)} rows so far...\")\n",
        "\n",
        "        # Check if we've reached max_rows\n",
        "        if max_rows and len(all_rows) >= max_rows:\n",
        "            all_rows = all_rows[:max_rows]\n",
        "            break\n",
        "\n",
        "    conn.close()\n",
        "    logger.info(f\"Total rows fetched: {len(all_rows)}\")\n",
        "\n",
        "    df = pd.DataFrame(all_rows, columns=[\"ENG\", \"YOR\", \"LIN\"])\n",
        "\n",
        "    yor_nan_count = df['YOR'].isna().sum()\n",
        "    lin_nan_count = df['LIN'].isna().sum()\n",
        "    logger.info(f\"Rows missing Yoruba: {yor_nan_count}, Rows missing Lingala: {lin_nan_count}\")\n",
        "\n",
        "    df = df.dropna(subset=['YOR', 'LIN'], how='all')\n",
        "    logger.info(f\"Rows after filtering: {len(df)}\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "dqhs4-XUHkYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# very new\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"Process a batch of examples for tokenization\"\"\"\n",
        "    inputs = examples[\"ENG\"]\n",
        "    targets = examples[\"target_text\"]\n",
        "    target_langs = examples[\"target_lang\"]\n",
        "\n",
        "    # Set source language\n",
        "    tokenizer.src_lang = \"eng_Latn\"\n",
        "\n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=128,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    # Process each example individually to set the correct target language\n",
        "    tokenized_targets = []\n",
        "    for i, target_lang in enumerate(target_langs):\n",
        "        # Set target language for this specific example\n",
        "        tokenizer.tgt_lang = target_lang\n",
        "\n",
        "        # Tokenize this target\n",
        "        with tokenizer.as_target_tokenizer():\n",
        "            tokenized_target = tokenizer(\n",
        "                targets[i],\n",
        "                max_length=128,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\"\n",
        "            )\n",
        "        tokenized_targets.append(tokenized_target[\"input_ids\"])\n",
        "\n",
        "    # Assign tokenized targets to model inputs\n",
        "    model_inputs[\"labels\"] = tokenized_targets\n",
        "\n",
        "    # Replace padding token id with -100\n",
        "    for i in range(len(model_inputs[\"labels\"])):\n",
        "        model_inputs[\"labels\"][i] = [\n",
        "            -100 if label == tokenizer.pad_token_id else label\n",
        "            for label in model_inputs[\"labels\"][i]\n",
        "        ]\n",
        "\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "qUBMk4pIZJ04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_datasets(df, test_size=0.1, val_size=0.1):\n",
        "    \"\"\"Split data and prepare datasets with stratified sampling\"\"\"\n",
        "    # Create a stratification column based on available translations\n",
        "    df['strat'] = df.apply(\n",
        "        lambda row: (\n",
        "            'both' if pd.notna(row['YOR']) and pd.notna(row['LIN']) else\n",
        "            'yor_only' if pd.notna(row['YOR']) else\n",
        "            'lin_only' if pd.notna(row['LIN']) else\n",
        "            'none'\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # First split into train and temp (test + validation)\n",
        "    train_df, temp_df = train_test_split(\n",
        "        df,\n",
        "        test_size=(test_size + val_size),\n",
        "        random_state=42,\n",
        "        stratify=df['strat']\n",
        "    )\n",
        "\n",
        "    # Split temp into validation and test\n",
        "    val_size_adjusted = val_size / (test_size + val_size)\n",
        "    val_df, test_df = train_test_split(\n",
        "        temp_df,\n",
        "        test_size=val_size_adjusted,\n",
        "        random_state=42,\n",
        "        stratify=temp_df['strat']\n",
        "    )\n",
        "\n",
        "    # Remove the stratification column\n",
        "    train_df = train_df.drop(columns=['strat'])\n",
        "    val_df = val_df.drop(columns=['strat'])\n",
        "    test_df = test_df.drop(columns=['strat'])\n",
        "\n",
        "    logger.info(f\"Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")\n",
        "\n",
        "    # Log distribution of languages in each split\n",
        "    for split_name, split_df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
        "        yor_count = split_df['YOR'].notna().sum()\n",
        "        lin_count = split_df['LIN'].notna().sum()\n",
        "        both_count = (split_df['YOR'].notna() & split_df['LIN'].notna()).sum()\n",
        "        logger.info(f\"{split_name} - Yoruba: {yor_count}, Lingala: {lin_count}, Both: {both_count}\")\n",
        "\n",
        "    return train_df, val_df, test_df"
      ],
      "metadata": {
        "id": "JTjlEPf1Hkfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_language_data(train_df, val_df, test_df):\n",
        "    \"\"\"Prepare language-specific datasets with validation\"\"\"\n",
        "    language_datasets = {}\n",
        "\n",
        "    # Process Yoruba data\n",
        "    for split_name, split_df in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n",
        "        # Create Yoruba datasets\n",
        "        df_yoruba = split_df[split_df[\"YOR\"].notna()].copy()\n",
        "        df_yoruba[\"target_lang\"] = \"yor_Latn\"\n",
        "        df_yoruba[\"target_text\"] = df_yoruba[\"YOR\"]\n",
        "        language_datasets[f\"yoruba_{split_name}\"] = df_yoruba\n",
        "\n",
        "        # Create Lingala datasets\n",
        "        df_lingala = split_df[split_df[\"LIN\"].notna()].copy()\n",
        "        df_lingala[\"target_lang\"] = \"lin_Latn\"\n",
        "        df_lingala[\"target_text\"] = df_lingala[\"LIN\"]\n",
        "        language_datasets[f\"lingala_{split_name}\"] = df_lingala\n",
        "\n",
        "    # Log dataset sizes\n",
        "    for dataset_name, dataset in language_datasets.items():\n",
        "        logger.info(f\"{dataset_name}: {len(dataset)} examples\")\n",
        "\n",
        "        # Validate a few examples to ensure alignment\n",
        "        if len(dataset) > 0:\n",
        "            sample = dataset.sample(min(3, len(dataset)))\n",
        "            for _, row in sample.iterrows():\n",
        "                source = row[\"ENG\"]\n",
        "                target = row[\"target_text\"]\n",
        "                logger.info(f\"Sample from {dataset_name}:\")\n",
        "                logger.info(f\"  Source: {source[:50]}...\")\n",
        "                logger.info(f\"  Target: {target[:50]}...\")\n",
        "\n",
        "    return language_datasets"
      ],
      "metadata": {
        "id": "SSU10WtFIJNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text, source_lang, target_lang):\n",
        "    \"\"\"Translate text from source_lang to target_lang\"\"\"\n",
        "    tokenizer.src_lang = source_lang\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Set forced BOS token to target language\n",
        "    forced_bos_token_id = tokenizer.convert_tokens_to_ids(target_lang)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            forced_bos_token_id=forced_bos_token_id,\n",
        "            max_length=128,\n",
        "        )\n",
        "\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "cQUKIKNkRxBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu_score(references, hypothesis):\n",
        "    \"\"\"Calculate BLEU score for a single translation\"\"\"\n",
        "    # Tokenize the sentences\n",
        "    tokenized_ref = [references.split()]\n",
        "    tokenized_hyp = hypothesis.split()\n",
        "\n",
        "    # Calculate BLEU score with smoothing\n",
        "    smooth = SmoothingFunction().method1\n",
        "    bleu_score = sentence_bleu(tokenized_ref, tokenized_hyp, smoothing_function=smooth)\n",
        "\n",
        "    return bleu_score"
      ],
      "metadata": {
        "id": "tEUhshMhRzBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sacrebleu_score(references, hypothesis):\n",
        "    \"\"\"Calculate sacreBLEU score for a single translation\"\"\"\n",
        "    bleu = sacrebleu.corpus_bleu([hypothesis], [[references]])\n",
        "    return bleu.score"
      ],
      "metadata": {
        "id": "fqkiFTqCR6LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_translations(model, tokenizer, test_df, source_lang, target_lang, lang_code):\n",
        "    \"\"\"Evaluate translations for a specific language pair\"\"\"\n",
        "    filtered_df = test_df[test_df[lang_code].notna()].copy()\n",
        "    if len(filtered_df) == 0:\n",
        "        logger.warning(f\"No test examples available for {target_lang}\")\n",
        "        return None\n",
        "\n",
        "    # Limit to a reasonable number for evaluation\n",
        "    if len(filtered_df) > 100:\n",
        "        filtered_df = filtered_df.sample(100, random_state=42)\n",
        "\n",
        "    bleu_scores = []\n",
        "    sacrebleu_scores = []\n",
        "    examples = []\n",
        "\n",
        "    logger.info(f\"Evaluating {len(filtered_df)} examples for {source_lang} → {target_lang}\")\n",
        "\n",
        "    for _, row in tqdm(filtered_df.iterrows(), total=len(filtered_df), desc=f\"Evaluating {target_lang}\"):\n",
        "        english = row[\"ENG\"]\n",
        "        expected = row[lang_code]\n",
        "\n",
        "        # Translate\n",
        "        translated = translate(english, source_lang, target_lang)\n",
        "\n",
        "        # Calculate scores\n",
        "        bleu = calculate_bleu_score(expected, translated)\n",
        "        sacrebleu = calculate_sacrebleu_score(expected, translated)\n",
        "\n",
        "        bleu_scores.append(bleu)\n",
        "        sacrebleu_scores.append(sacrebleu)\n",
        "\n",
        "        # Save example for display\n",
        "        examples.append({\n",
        "            \"source\": english,\n",
        "            \"expected\": expected,\n",
        "            \"translated\": translated,\n",
        "            \"bleu\": bleu,\n",
        "            \"sacrebleu\": sacrebleu\n",
        "        })\n",
        "\n",
        "    # Calculate average scores\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
        "    avg_sacrebleu = sum(sacrebleu_scores) / len(sacrebleu_scores) if sacrebleu_scores else 0\n",
        "\n",
        "    logger.info(f\"Average BLEU score for {target_lang}: {avg_bleu:.4f}\")\n",
        "    logger.info(f\"Average sacreBLEU score for {target_lang}: {avg_sacrebleu:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"language\": target_lang,\n",
        "        \"avg_bleu\": avg_bleu,\n",
        "        \"avg_sacrebleu\": avg_sacrebleu,\n",
        "        \"examples\": examples\n",
        "    }"
      ],
      "metadata": {
        "id": "jp1T29auR9e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "# ready\n",
        "def main():\n",
        "    # Check if cached datasets exist\n",
        "    if (os.path.exists(TRAIN_CACHE) and\n",
        "        os.path.exists(VAL_CACHE) and\n",
        "        os.path.exists(TEST_CACHE) and\n",
        "        os.path.exists(\"./cached_datasets/test_df.pkl\") and\n",
        "        os.environ.get('REPROCESS_DATA') != '1'):\n",
        "\n",
        "        logger.info(\"Loading cached datasets\")\n",
        "        train_dataset = load_from_disk(TRAIN_CACHE)\n",
        "        val_dataset = load_from_disk(VAL_CACHE)\n",
        "        test_dataset = load_from_disk(TEST_CACHE)\n",
        "\n",
        "        # Load test dataframe for evaluation\n",
        "        test_df = pd.read_pickle(\"./cached_datasets/test_df.pkl\")\n",
        "    else:\n",
        "        # Set to None to process all rows, or a number to limit during development\n",
        "        max_rows = None  # e.g., 10000 for faster development\n",
        "\n",
        "        # Fetch data with ordered IDs\n",
        "        df = fetch_data_in_batches(max_rows=max_rows)\n",
        "\n",
        "        # Prepare datasets with stratified sampling\n",
        "        train_df, val_df, test_df = prepare_datasets(df)\n",
        "\n",
        "        # Save test dataframe for later evaluation\n",
        "        os.makedirs(\"./cached_datasets\", exist_ok=True)\n",
        "        test_df.to_pickle(\"./cached_datasets/test_df.pkl\")\n",
        "\n",
        "        # Prepare language-specific data\n",
        "        lang_data = prepare_language_data(train_df, val_df, test_df)\n",
        "\n",
        "        # Convert to Hugging Face datasets\n",
        "        logger.info(\"Converting to Hugging Face datasets\")\n",
        "        yoruba_train_dataset = Dataset.from_pandas(lang_data[\"yoruba_train\"])\n",
        "        lingala_train_dataset = Dataset.from_pandas(lang_data[\"lingala_train\"])\n",
        "        yoruba_val_dataset = Dataset.from_pandas(lang_data[\"yoruba_val\"])\n",
        "        lingala_val_dataset = Dataset.from_pandas(lang_data[\"lingala_val\"])\n",
        "        yoruba_test_dataset = Dataset.from_pandas(lang_data[\"yoruba_test\"])\n",
        "        lingala_test_dataset = Dataset.from_pandas(lang_data[\"lingala_test\"])\n",
        "\n",
        "        # Process the datasets with multiple workers\n",
        "        logger.info(\"Processing training datasets\")\n",
        "        yoruba_train_dataset = yoruba_train_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            batch_size=64,\n",
        "            num_proc=4,\n",
        "            remove_columns=[\"ENG\", \"YOR\", \"LIN\", \"target_lang\", \"target_text\"]\n",
        "        )\n",
        "\n",
        "        lingala_train_dataset = lingala_train_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            batch_size=64,\n",
        "            num_proc=4,\n",
        "            remove_columns=[\"ENG\", \"YOR\", \"LIN\", \"target_lang\", \"target_text\"]\n",
        "        )\n",
        "\n",
        "        logger.info(\"Processing validation datasets\")\n",
        "        yoruba_val_dataset = yoruba_val_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            batch_size=64,\n",
        "            num_proc=4,\n",
        "            remove_columns=[\"ENG\", \"YOR\", \"LIN\", \"target_lang\", \"target_text\"]\n",
        "        )\n",
        "\n",
        "        lingala_val_dataset = lingala_val_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            batch_size=64,\n",
        "            num_proc=4,\n",
        "            remove_columns=[\"ENG\", \"YOR\", \"LIN\", \"target_lang\", \"target_text\"]\n",
        "        )\n",
        "\n",
        "        logger.info(\"Processing test datasets\")\n",
        "        yoruba_test_dataset = yoruba_test_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            batch_size=64,\n",
        "            num_proc=4,\n",
        "            remove_columns=[\"ENG\", \"YOR\", \"LIN\", \"target_lang\", \"target_text\"]\n",
        "        )\n",
        "\n",
        "        lingala_test_dataset = lingala_test_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            batch_size=64,\n",
        "            num_proc=4,\n",
        "            remove_columns=[\"ENG\", \"YOR\", \"LIN\", \"target_lang\", \"target_text\"]\n",
        "        )\n",
        "\n",
        "        # Combine datasets\n",
        "        train_dataset = concatenate_datasets([yoruba_train_dataset, lingala_train_dataset])\n",
        "        val_dataset = concatenate_datasets([yoruba_val_dataset, lingala_val_dataset])\n",
        "        test_dataset = concatenate_datasets([yoruba_test_dataset, lingala_test_dataset])\n",
        "\n",
        "        # Cache the datasets\n",
        "        logger.info(\"Caching processed datasets\")\n",
        "        train_dataset.save_to_disk(TRAIN_CACHE)\n",
        "        val_dataset.save_to_disk(VAL_CACHE)\n",
        "        test_dataset.save_to_disk(TEST_CACHE)\n",
        "\n",
        "    # Set up training arguments\n",
        "    logger.info(\"Setting up training arguments\")\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=\"./nllb-finetuned\",\n",
        "        evaluation_strategy=\"steps\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        gradient_accumulation_steps=4,\n",
        "        gradient_checkpointing=True,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        save_total_limit=2,\n",
        "        save_steps=500,\n",
        "        eval_steps=500,\n",
        "        logging_steps=100,\n",
        "        predict_with_generate=True,\n",
        "        bf16=True,  # ✅ Enable fp16 only if CUDA is available\n",
        "        logging_dir=\"./logs\",\n",
        "        report_to=\"tensorboard\",\n",
        "        push_to_hub=False,  # Set to True if you want to push to HF Hub\n",
        "    )\n",
        "\n",
        "    # Set up data collator\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer,\n",
        "        model=model,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    # Set up trainer\n",
        "    logger.info(\"Setting up trainer\")\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    logger.info(\"Starting training\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model\n",
        "    logger.info(\"Saving model\")\n",
        "    trainer.save_model(\"./nllb-finetuned-final\")\n",
        "\n",
        "    # Return the trainer and test_df for further evaluation\n",
        "    return trainer, test_df"
      ],
      "metadata": {
        "id": "2ht94yxrSPcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer, test_df = main()"
      ],
      "metadata": {
        "id": "bXaaihlnSTXj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "10972ff4-6f9e-4b45-af14-72629c8da23c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-4abf26fb6a23>:141: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='280' max='5934' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 280/5934 06:16 < 2:07:33, 0.74 it/s, Epoch 0.14/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "help(concatenate_datasets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioJmJj89dRf3",
        "outputId": "171fe37f-0cb5-4780-b8b7-d6e92f15b0e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function concatenate_datasets in module datasets.combine:\n",
            "\n",
            "concatenate_datasets(dsets: list[~DatasetType], info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None, axis: int = 0) -> ~DatasetType\n",
            "    Converts a list of [`Dataset`] with the same schema into a single [`Dataset`].\n",
            "    \n",
            "    Args:\n",
            "        dsets (`List[datasets.Dataset]`):\n",
            "            List of Datasets to concatenate.\n",
            "        info (`DatasetInfo`, *optional*):\n",
            "            Dataset information, like description, citation, etc.\n",
            "        split (`NamedSplit`, *optional*):\n",
            "            Name of the dataset split.\n",
            "        axis (`{0, 1}`, defaults to `0`):\n",
            "            Axis to concatenate over, where `0` means over rows (vertically) and `1` means over columns\n",
            "            (horizontally).\n",
            "    \n",
            "            <Added version=\"1.6.0\"/>\n",
            "    \n",
            "    Example:\n",
            "    \n",
            "    ```py\n",
            "    >>> ds3 = concatenate_datasets([ds1, ds2])\n",
            "    ```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"Evaluating model on test set\")\n",
        "evaluation_results = {}\n",
        "\n",
        "# Evaluate Yoruba translations\n",
        "yoruba_results = evaluate_translations(\n",
        "    model, tokenizer, test_df,\n",
        "    source_lang=\"eng_Latn\", target_lang=\"yor_Latn\", lang_code=\"YOR\"\n",
        ")\n",
        "if yoruba_results:\n",
        "    evaluation_results[\"yoruba\"] = yoruba_results\n",
        "\n",
        "# Evaluate Lingala translations\n",
        "lingala_results = evaluate_translations(\n",
        "    model, tokenizer, test_df,\n",
        "    source_lang=\"eng_Latn\", target_lang=\"lin_Latn\", lang_code=\"LIN\"\n",
        ")\n",
        "if lingala_results:\n",
        "    evaluation_results[\"lingala\"] = lingala_results\n",
        "\n",
        "# Print detailed evaluation results\n",
        "print(\"\\n==== EVALUATION RESULTS ====\")\n",
        "for lang, results in evaluation_results.items():\n",
        "    print(f\"\\n{lang.upper()} TRANSLATION METRICS:\")\n",
        "    print(f\"Average BLEU score: {results['avg_bleu']:.4f}\")\n",
        "    print(f\"Average sacreBLEU score: {results['avg_sacrebleu']:.4f}\")\n",
        "\n",
        "    print(f\"\\n{lang.upper()} TRANSLATION EXAMPLES:\")\n",
        "    # Print 5 examples with their metrics\n",
        "    for i, example in enumerate(results['examples'][:5]):\n",
        "        print(f\"Example {i+1}:\")\n",
        "        print(f\"Source: {example['source']}\")\n",
        "        print(f\"Expected: {example['expected']}\")\n",
        "        print(f\"Translated: {example['translated']}\")\n",
        "        print(f\"BLEU: {example['bleu']:.4f}, sacreBLEU: {example['sacrebleu']:.4f}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "logger.info(\"Training and evaluation complete\")"
      ],
      "metadata": {
        "id": "-dWbYcM1SYb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_translate():\n",
        "    \"\"\"Interactive function to test translations\"\"\"\n",
        "    print(\"Interactive Translation Testing\")\n",
        "    print(\"Type 'exit' to quit\")\n",
        "\n",
        "    while True:\n",
        "        text = input(\"\\nEnter English text to translate: \")\n",
        "        if text.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        target = input(\"Translate to (yor/lin): \").lower()\n",
        "        if target == 'yor':\n",
        "            target_lang = \"yor_Latn\"\n",
        "        elif target == 'lin':\n",
        "            target_lang = \"lin_Latn\"\n",
        "        else:\n",
        "            print(\"Invalid language. Use 'yor' or 'lin'.\")\n",
        "            continue\n",
        "\n",
        "        translation = translate(text, \"eng_Latn\", target_lang)\n",
        "        print(f\"\\nTranslation: {translation}\")"
      ],
      "metadata": {
        "id": "xKe1WIZNSbKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interactive_translate()"
      ],
      "metadata": {
        "id": "tMKkxg_BScBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cZ0m5YKuSd3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "wEL1Czu7RxrF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6G0qPWhARxSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Check if cached datasets exist\n",
        "    if os.path.exists(TRAIN_CACHE) and os.path.exists(VAL_CACHE):\n",
        "        logger.info(\"Loading cached datasets\")\n",
        "        train_dataset = load_from_disk(TRAIN_CACHE)\n",
        "        val_dataset = load_from_disk(VAL_CACHE)\n",
        "    else:\n",
        "        # Set to None to process all rows, or a number to limit during development\n",
        "        max_rows = None  # e.g., 10000 for faster development\n",
        "\n",
        "        # Fetch data\n",
        "        df = fetch_data_in_batches(batch_size=10000, max_rows=max_rows)\n",
        "\n",
        "        # Prepare datasets\n",
        "        train_df, val_df, test_df = prepare_datasets(df)\n",
        "\n",
        "        # Prepare language-specific data\n",
        "        lang_data = prepare_language_data(train_df, val_df)\n",
        "\n",
        "        # Convert to Hugging Face datasets\n",
        "        logger.info(\"Converting to Hugging Face datasets\")\n",
        "        yoruba_train_dataset = Dataset.from_pandas(lang_data[\"yoruba_train\"])\n",
        "        lingala_train_dataset = Dataset.from_pandas(lang_data[\"lingala_train\"])\n",
        "        yoruba_val_dataset = Dataset.from_pandas(lang_data[\"yoruba_val\"])\n",
        "        lingala_val_dataset = Dataset.from_pandas(lang_data[\"lingala_val\"])\n",
        "\n",
        "        # Process the datasets with multiple workers\n",
        "        logger.info(\"Processing training datasets\")\n",
        "        yoruba_train_dataset = yoruba_train_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            batch_size=64,\n",
        "            num_proc=4,\n",
        "            remove_columns=[\"ENG\", \"YOR\", \"LIN\", \"target_lang\", \"target_text\"]\n",
        "        )\n",
        "\n",
        "        lingala_train_dataset = lingala_train_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            batch_size=64,\n",
        "            num_proc=4,\n",
        "            remove_columns=[\"ENG\", \"YOR\", \"LIN\", \"target_lang\", \"target_text\"]\n",
        "        )\n",
        "\n",
        "        logger.info(\"Processing validation datasets\")\n",
        "        yoruba_val_dataset = yoruba_val_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            batch_size=64,\n",
        "            num_proc=4,\n",
        "            remove_columns=[\"ENG\", \"YOR\", \"LIN\", \"target_lang\", \"target_text\"]\n",
        "        )\n",
        "\n",
        "        lingala_val_dataset = lingala_val_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            batch_size=64,\n",
        "            num_proc=4,\n",
        "            remove_columns=[\"ENG\", \"YOR\", \"LIN\", \"target_lang\", \"target_text\"]\n",
        "        )\n",
        "\n",
        "        # Combine datasets\n",
        "        train_dataset = concatenate_datasets([yoruba_train_dataset, lingala_train_dataset])\n",
        "        val_dataset = concatenate_datasets([yoruba_val_dataset, lingala_val_dataset])\n",
        "\n",
        "        # Cache the datasets\n",
        "        logger.info(\"Caching processed datasets\")\n",
        "        train_dataset.save_to_disk(TRAIN_CACHE)\n",
        "        val_dataset.save_to_disk(VAL_CACHE)\n",
        "\n",
        "    # Set up training arguments\n",
        "    logger.info(\"Setting up training arguments\")\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=\"./nllb-finetuned\",\n",
        "        evaluation_strategy=\"steps\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        gradient_accumulation_steps=4,\n",
        "        gradient_checkpointing=True,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        save_total_limit=2,\n",
        "        save_steps=500,\n",
        "        eval_steps=500,\n",
        "        logging_steps=100,\n",
        "        predict_with_generate=True,\n",
        "        fp16=device == \"cuda\",  # Enable mixed precision only on CUDA\n",
        "        logging_dir=\"./logs\",\n",
        "        report_to=\"tensorboard\",\n",
        "        push_to_hub=False,  # Set to True if you want to push to HF Hub\n",
        "    )\n",
        "\n",
        "    # Set up data collator\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer,\n",
        "        model=model,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    # Set up trainer\n",
        "    logger.info(\"Setting up trainer\")\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    logger.info(\"Starting training\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model\n",
        "    logger.info(\"Saving model\")\n",
        "    trainer.save_model(\"./nllb-finetuned-final\")"
      ],
      "metadata": {
        "id": "lk31q2LJIIuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on a sample from test set\n",
        "def translate(text, source_lang, target_lang):\n",
        "        tokenizer.src_lang = source_lang\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Set forced BOS token to target language\n",
        "        forced_bos_token_id = tokenizer.convert_tokens_to_ids(target_lang)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                forced_bos_token_id=forced_bos_token_id,\n",
        "                max_length=128,\n",
        "            )\n",
        "\n",
        "        return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "-yFQAcqqIWba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"Evaluation on test examples:\")\n",
        "\n",
        "    # Recreate test set if we don't have it\n",
        "if not 'test_df' in locals():\n",
        "        # Fetch a small sample for testing\n",
        "        test_df = fetch_data_in_batches(batch_size=100, max_rows=100)\n",
        "\n",
        "    # Sample a few examples\n",
        "test_samples = test_df.sample(min(5, len(test_df)))\n",
        "\n",
        "print(\"\\nTranslation Examples (English → Yoruba):\")\n",
        "for _, row in test_samples.iterrows():\n",
        "      if pd.notna(row[\"YOR\"]):\n",
        "            english = row[\"ENG\"]\n",
        "            expected_yoruba = row[\"YOR\"]\n",
        "            translated = translate(english, \"eng_Latn\", \"yor_Latn\")\n",
        "\n",
        "            print(f\"Source: {english}\")\n",
        "            print(f\"Expected: {expected_yoruba}\")\n",
        "            print(f\"Translated: {translated}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "print(\"\\nTranslation Examples (English → Lingala):\")\n",
        "for _, row in test_samples.iterrows():\n",
        "      if pd.notna(row[\"LIN\"]):\n",
        "            english = row[\"ENG\"]\n",
        "            expected_lingala = row[\"LIN\"]\n",
        "            translated = translate(english, \"eng_Latn\", \"lin_Latn\")\n",
        "\n",
        "            print(f\"Source: {english}\")\n",
        "            print(f\"Expected: {expected_lingala}\")\n",
        "            print(f\"Translated: {translated}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "logger.info(\"Training and evaluation complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLwz9_MWIWxw",
        "outputId": "5fd030de-b509-4ce7-a15d-d2ab6bab162c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Translation Examples (English → Yoruba):\n",
            "Source: (The gold of that land is good; aromatic resin\n",
            "Expected: “Mo ṣẹ̀ṣẹ̀ rí ẹni tí ó dàbí mi,\n",
            "Translated: (Gólọ́ọ̀lù ilẹ̀ yẹn dára; òwú òórùn\n",
            "--------------------------------------------------\n",
            "Source: God formed a man\n",
            "Expected: Orúkọ odò kẹta ni Tigirisi, òun ni ó ṣàn lọ sí apá ìlà oòrùn Asiria. Ẹkẹrin ni odò Yufurate.\n",
            "Translated: Ọlọ́run dá èèyàn\n",
            "--------------------------------------------------\n",
            "Source: God had not sent rain on the earth and there was no one to work the ground,\n",
            "Expected: Láti inú ilẹ̀ ni OLUWA Ọlọrun ti mú kí oríṣìíríṣìí igi hù jáde ninu ọgbà náà, tí wọ́n dùn ún wò, tí wọ́n sì dára fún jíjẹ. Igi ìyè wà láàrin ọgbà náà, ati igi ìmọ̀ ibi ati ire.\n",
            "Translated: Ọlọ́run kò tíì rọ̀jò lórí ilẹ̀ ayé, kò sì sí ẹni tó lè ṣiṣẹ́ lórí ilẹ̀ náà.\n",
            "--------------------------------------------------\n",
            "Source: God had planted a garden in the east, in Eden; and there he put the man he had formed.\n",
            "Expected: Lẹ́yìn náà OLUWA Ọlọrun sọ pé, “Kò dára kí ọkunrin náà nìkan dá wà, n óo ṣe olùrànlọ́wọ́ kan fún un, tí yóo dàbí rẹ̀.”\n",
            "Translated: Ọlọrun gbìn ọgbà kan ní ìlà oòrùn, ni Édẹ́nì, ó sì fi ọkùnrin tí ó dá sí ibẹ̀.\n",
            "--------------------------------------------------\n",
            "Source: Thus the heavens and the earth were completed in all their vast array.\n",
            "Expected: Bẹ́ẹ̀ ni Ọlọrun ṣe parí dídá ọ̀run ati ayé, ati gbogbo ohun tí ó wà ninu wọn.\n",
            "Translated: Bí ọ̀run àti ilẹ̀ ayé ṣe kún fún gbogbo ohun tí wọ́n ní.\n",
            "--------------------------------------------------\n",
            "\n",
            "Translation Examples (English → Lingala):\n",
            "Source: (The gold of that land is good; aromatic resin\n",
            "Expected: « Tala na mbala oyo,\n",
            "Translated: (Boli ya mokili yango ezali malamu; mafuta ya nsolo kitoko\n",
            "--------------------------------------------------\n",
            "Source: God formed a man\n",
            "Expected: Eteni ya misato, kombo na yango « Tigre. » Yango elekana na este ya Asiri. Mpe eteni ya minei, kombo na yango « Efrate. »\n",
            "Translated: Nzambe asalaki moto\n",
            "--------------------------------------------------\n",
            "Source: God had not sent rain on the earth and there was no one to work the ground,\n",
            "Expected: Yawe Nzambe abimisaki wuta na mabele banzete ya lolenge nyonso oyo ezalaki kitoko mpo na kotala mpe elengi mpo na kolia. Atiaki mpe nzete ya bomoi na kati-kati ya elanga ; lisusu atiaki nzete ya koyeba malamu mpe mabe.\n",
            "Translated: Nzambe atindaki mbula na mokili te mpe moto moko te azalaki kosala mosala na mabele.\n",
            "--------------------------------------------------\n",
            "Source: God had planted a garden in the east, in Eden; and there he put the man he had formed.\n",
            "Expected: Yawe Nzambe alobaki : « Ezali malamu te ete mobali azala ye moko ; nakosalela ye mosungi oyo azali ndenge moko na ye. »\n",
            "Translated: Nzambe alonaki elanga na Ɛdi, na ɛsti; mpe kuna atyaki moto oyo ye asalaki.\n",
            "--------------------------------------------------\n",
            "Source: Thus the heavens and the earth were completed in all their vast array.\n",
            "Expected: Ezalaki bongo nde likolo mpe mokili ekelamaki elongo na biloko nyonso oyo ezali kati na yango.\n",
            "Translated: Na bongo likoló mpe mabelé na molɔngɔ na yango nyonso ya monene ekokisamaki.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U_eA5b1TIXLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LvOdTmwNIXWQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}